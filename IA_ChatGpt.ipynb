{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e026950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. On charge les variables du .env\n",
    "load_dotenv()\n",
    "\n",
    "FICHIER_AVIS = os.getenv(\"INPUT_REVIEWS\")\n",
    "\n",
    "df = pd.read_json(FICHIER_AVIS, lines=True, chunksize=50000)\n",
    "df = next(df) # On ne prend que le premier bloc de 50k avis\n",
    "\n",
    "# 2. Création de la target 1 : Polarité (Sentiment)\n",
    "def get_sentiment(stars):\n",
    "    if stars > 3: return 2 # Positif (Mapping numérique pour les modèles)\n",
    "    elif stars < 3: return 0 # Négatif\n",
    "    else: return 1 # Neutre\n",
    "\n",
    "df['label_sentiment'] = df['stars'].apply(get_sentiment)\n",
    "\n",
    "# 3. Target 2 : Score (Rating)\n",
    "# Déjà présent dans 'stars' (1 à 5). \n",
    "# Note : Pour les modèles, il vaut mieux souvent décaler à 0-4 (donc stars - 1)\n",
    "df['label_rating'] = df['stars'] - 1 \n",
    "\n",
    "# 4. Nettoyage basique (optionnel mais conseillé)\n",
    "# Minuscules, suppression ponctuation...\n",
    "df['clean_text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44d56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label_sentiment'] # Ou label_rating selon la tâche\n",
    "\n",
    "# Séparation Train / Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 1. Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=5000) # On limite à 5000 mots pour la mémoire\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\floco\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\floco\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\floco\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 168.48it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Batches:  22%|██▏       | 280/1250 [19:20<1:07:28,  4.17s/it]"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_emb = SentenceTransformer('all-MiniLM-L6-v2') # Modèle rapide et performant\n",
    "\n",
    "# Cela convertit les phrases en vecteurs de 384 dimensions\n",
    "X_train_emb = model_emb.encode(X_train.tolist(), show_progress_bar=True)\n",
    "X_test_emb = model_emb.encode(X_test.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Exemple Régression Logistique sur TF-IDF\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression + TF-IDF :\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8abf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Exemple simple MLP (Multi-Layer Perceptron)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(384,)), # 384 si vous utilisez les embeddings MiniLM, 5000 si TF-IDF\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation='softmax') # 3 sorties pour (Neg, Neutre, Pos)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_emb, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd65e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"distilbert-base-uncased\" # Plus léger que BERT standard\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Il faudra créer un Dataset compatible HuggingFace ici...\n",
    "# (C'est souvent la partie qui demande un peu plus de code de formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175755e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept : On donne le texte et on demande la réponse\n",
    "prompt = f\"\"\"\n",
    "Analyse le sentiment de cet avis Yelp.\n",
    "Avis : \"{un_avis_du_dataset}\"\n",
    "Réponds uniquement par : POSITIF, NÉGATIF ou NEUTRE.\n",
    "\"\"\"\n",
    "# Envoyer ce prompt à l'API et comparer la réponse avec la vraie note (stars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

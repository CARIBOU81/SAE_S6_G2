{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e026950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers utilisés : {'tout': 'C:/Users/floco/OneDrive/Documents/S6/Sae/yelp_academic_reviews4students.jsonl', 'Health_Medical': 'C:/Users/floco/OneDrive/Documents/S6/Sae/donnees_triees/yelp_Health_Medical.json', 'Hotels': 'C:/Users/floco/OneDrive/Documents/S6/Sae/donnees_triees/yelp_Hotels.json', 'Restaurants': 'C:/Users/floco/OneDrive/Documents/S6/Sae/donnees_triees/yelp_Restaurants.json', 'Shopping': 'C:/Users/floco/OneDrive/Documents/S6/Sae/donnees_triees/yelp_Shopping.jsonl'}\n",
      "\n",
      "=============================\n",
      " MODELE POUR : TOUT\n",
      "=============================\n",
      "Nb lignes : 1000000\n",
      "stars\n",
      "5    462646\n",
      "4    207953\n",
      "1    153057\n",
      "3     98714\n",
      "2     77630\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "FICHIER_AVIS = os.getenv(\"INPUT_REVIEWS\")\n",
    "FICHIER_BUSINESS = os.getenv(\"INPUT_BUSINESS\")\n",
    "FICHIER_SORTIE = os.getenv(\"OUTPUT_FILE\")\n",
    "\n",
    "FILES = {\n",
    "    \"tout\": FICHIER_AVIS,\n",
    "    \"Health_Medical\": os.getenv(\"HEALTH_MEDICAL\"),\n",
    "    \"Hotels\": os.getenv(\"HOTELS\"),\n",
    "    \"Restaurants\": os.getenv(\"RESTAURANTS\"),\n",
    "    \"Shopping\": os.getenv(\"SHOPPING\"),\n",
    "}\n",
    "\n",
    "print(\"Fichiers utilisés :\", FILES)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FONCTIONS UTILES\n",
    "# =========================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñæœ\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = []\n",
    "\n",
    "    # Vérification simple si le fichier existe\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(f\"⚠️ Fichier introuvable : {path}\")\n",
    "        return pd.DataFrame(columns=[\"text\", \"stars\"])\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                # La magie opère ici : json.loads comprend automatiquement le format\n",
    "                review = json.loads(line)\n",
    "\n",
    "                # On extrait proprement\n",
    "                data.append({\n",
    "                    \"text\": review.get(\"text\", \"\"), # .get évite le crash si la clé manque\n",
    "                    \"stars\": review.get(\"stars\", 0)\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                continue # On saute juste la ligne si elle est illisible\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BOUCLE PRINCIPALE\n",
    "# =========================\n",
    "\n",
    "for etablissement, filepath in FILES.items():\n",
    "\n",
    "    print(f\"\\n=============================\")\n",
    "    print(f\" MODELE POUR : {etablissement.upper()}\")\n",
    "    print(f\"=============================\")\n",
    "\n",
    "    # 1. Chargement\n",
    "    df = load_dataset(filepath)\n",
    "    print(\"Nb lignes :\", len(df))\n",
    "    print(df[\"stars\"].value_counts())\n",
    "\n",
    "    # 2. Nettoyage\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    # (optionnel) réduire pour aller plus vite\n",
    "    df = df.sample(200000, random_state=42)\n",
    "\n",
    "    # 3. Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[\"clean_text\"],\n",
    "        df[\"stars\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df[\"stars\"]\n",
    "    )\n",
    "\n",
    "    # 4. TF-IDF (UN par type)\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=200000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    # 5. Modèle\n",
    "    model = svm.SVC(kernel='linear')\n",
    "\n",
    "    model.fit(X_train_vec, y_train)\n",
    "\n",
    "    # 6. Évaluation\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "\n",
    "    print(\"\\n--- RESULTATS ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    accuracy = (y_test == y_pred).mean()\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "\n",
    "    # 7. Sauvegarde prédictions\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"true_stars\": y_test.values,\n",
    "        \"predicted_stars\": y_pred\n",
    "    })\n",
    "\n",
    "    output_file = f\"predictions_{etablissement}.csv\"\n",
    "    df_pred.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Fichier sauvegardé : {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44d56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label_sentiment'] # Ou label_rating selon la tâche\n",
    "\n",
    "# Séparation Train / Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 1. Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=5000) # On limite à 5000 mots pour la mémoire\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4778ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flocon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\flocon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\flocon\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 534.96it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Batches: 100%|██████████| 1250/1250 [10:14<00:00,  2.03it/s]\n",
      "Batches: 100%|██████████| 313/313 [02:41<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_emb = SentenceTransformer('all-MiniLM-L6-v2') # Modèle rapide et performant\n",
    "\n",
    "# Cela convertit les phrases en vecteurs de 384 dimensions\n",
    "X_train_emb = model_emb.encode(X_train.tolist(), show_progress_bar=True)\n",
    "X_test_emb = model_emb.encode(X_test.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd74aaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + TF-IDF :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84      2297\n",
      "           1       0.48      0.24      0.32       950\n",
      "           2       0.90      0.96      0.93      6753\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.74      0.68      0.70     10000\n",
      "weighted avg       0.85      0.86      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Exemple Régression Logistique sur TF-IDF\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression + TF-IDF :\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8abf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8200 - loss: 0.4732\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.4186\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4068\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.3971\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8516 - loss: 0.3890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a30feb7620>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Exemple simple MLP (Multi-Layer Perceptron)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(384,)), # 384 si vous utilisez les embeddings MiniLM, 5000 si TF-IDF\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation='softmax') # 3 sorties pour (Neg, Neutre, Pos)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_emb, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd65e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flocon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\flocon\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 677.09it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"distilbert-base-uncased\" # Plus léger que BERT standard\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Il faudra créer un Dataset compatible HuggingFace ici...\n",
    "# (C'est souvent la partie qui demande un peu plus de code de formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175755e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'un_avis_du_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Concept : On donne le texte et on demande la réponse\u001b[39;00m\n\u001b[32m      2\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mAnalyse le sentiment de cet avis Yelp.\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[33mAvis : \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mun_avis_du_dataset\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mRéponds uniquement par : POSITIF, NÉGATIF ou NEUTRE.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Envoyer ce prompt à l'API et comparer la réponse avec la vraie note (stars)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'un_avis_du_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Concept : On donne le texte et on demande la réponse\n",
    "prompt = f\"\"\"\n",
    "Analyse le sentiment de cet avis Yelp.\n",
    "Avis : \"{un_avis_du_dataset}\"\n",
    "Réponds uniquement par : POSITIF, NÉGATIF ou NEUTRE.\n",
    "\"\"\"\n",
    "# Envoyer ce prompt à l'API et comparer la réponse avec la vraie note (stars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
